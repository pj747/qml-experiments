{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNL/9qQvBP+3aYQjeoSoKCi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pj747/qml-experiments/blob/main/QML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJTVCV_H-Qq3",
        "outputId": "de6790fb-a5e3-41a6-fb50-037524ba9c3a"
      },
      "source": [
        "\n",
        "!pip install wandb\n",
        "!wandb login\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/48/b199e2b3b341ac842108c5db4956091dd75d961cfa77aceb033e99cac20f/wandb-0.10.31-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 18.2MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 17.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathtools, subprocess32\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=7f75d2949c3760dc1f481d168997be6420bd459e4365415eadb02439c5caca97\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=0de46e59da9c22dbf45e2efb5c3ccd76df2d321f30f691c5989223577547cb99\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built pathtools subprocess32\n",
            "Installing collected packages: smmap, gitdb, GitPython, shortuuid, sentry-sdk, configparser, docker-pycreds, pathtools, subprocess32, wandb\n",
            "Successfully installed GitPython-3.1.17 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOfZ7Oiwaief",
        "outputId": "83917b4e-b4cc-4d3b-807f-b101b4d07572"
      },
      "source": [
        "!pip install pennylane --upgrade\n",
        "!pip install pennylane-qulacs[\"gpu\"] --upgrade"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pennylane\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/e3/be051bad48308df6bae15b5447be22cb814a06098afbd8eb507d20196025/PennyLane-0.15.1-py3-none-any.whl (455kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: appdirs in /usr/local/lib/python3.7/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.7/dist-packages (from pennylane) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from pennylane) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from pennylane) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: toml in /usr/local/lib/python3.7/dist-packages (from pennylane) (0.10.2)\n",
            "Collecting semantic-version==2.6\n",
            "  Downloading https://files.pythonhosted.org/packages/28/be/3a7241d731ba89063780279a5433f5971c1cf41735b64a9f874b7c3ff995/semantic_version-2.6.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: autograd in /usr/local/lib/python3.7/dist-packages (from pennylane) (1.3)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->pennylane) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd->pennylane) (0.16.0)\n",
            "Installing collected packages: semantic-version, pennylane\n",
            "Successfully installed pennylane-0.15.1 semantic-version-2.6.0\n",
            "Collecting pennylane-qulacs[gpu]\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/cc/ed8c79b56325f1d4ae2eb8e3801e21ade8290d5a440e49366619c8814f25/pennylane_qulacs-0.15.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: pennylane>=0.15 in /usr/local/lib/python3.7/dist-packages (from pennylane-qulacs[gpu]) (0.15.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from pennylane-qulacs[gpu]) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from pennylane-qulacs[gpu]) (1.19.5)\n",
            "Collecting qulacs-gpu>=0.1.10.1; extra == \"gpu\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/2a/b6e34393074d184eb251ccafd0c5f76d65f213d1ffe156197d10c69b80bd/Qulacs-GPU-0.2.0.tar.gz (206kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: semantic-version==2.6 in /usr/local/lib/python3.7/dist-packages (from pennylane>=0.15->pennylane-qulacs[gpu]) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: autograd in /usr/local/lib/python3.7/dist-packages (from pennylane>=0.15->pennylane-qulacs[gpu]) (1.3)\n",
            "Requirement already satisfied, skipping upgrade: toml in /usr/local/lib/python3.7/dist-packages (from pennylane>=0.15->pennylane-qulacs[gpu]) (0.10.2)\n",
            "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.7/dist-packages (from pennylane>=0.15->pennylane-qulacs[gpu]) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: appdirs in /usr/local/lib/python3.7/dist-packages (from pennylane>=0.15->pennylane-qulacs[gpu]) (1.4.4)\n",
            "Requirement already satisfied, skipping upgrade: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd->pennylane>=0.15->pennylane-qulacs[gpu]) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->pennylane>=0.15->pennylane-qulacs[gpu]) (4.4.2)\n",
            "Building wheels for collected packages: qulacs-gpu\n",
            "  Building wheel for qulacs-gpu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qulacs-gpu: filename=Qulacs_GPU-0.2.0-cp37-cp37m-linux_x86_64.whl size=884821 sha256=208ee947821822891797f060defc4aaf32aebbad907140e85b78b7e2019a9d94\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/c6/90/6f34fb74a323135ff90fa1a844f9fd5cbc5c6d98edaf9c347f\n",
            "Successfully built qulacs-gpu\n",
            "Installing collected packages: qulacs-gpu, pennylane-qulacs\n",
            "Successfully installed pennylane-qulacs-0.15.0 qulacs-gpu-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEP3SSThY1Gl"
      },
      "source": [
        "import pennylane as qml"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrVSOd9K95-l"
      },
      "source": [
        "\n",
        "import math\n",
        "import wandb\n",
        "from pennylane import numpy as np\n",
        "from types import SimpleNamespace\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw-3i-hy99AC"
      },
      "source": [
        "dataSet = load_breast_cancer()\n",
        "X = dataSet.data\n",
        "Y = dataSet.target\n",
        "Y = Y * 2 - np.ones(len(Y))\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9S_8F759tUR"
      },
      "source": [
        "def accuracy(labels, predictions):\n",
        "    accuracy = 0\n",
        "    for l, p in zip(labels, predictions):\n",
        "        if abs(l - p) < 1e-5:\n",
        "            accuracy = accuracy + 1\n",
        "    accuracy = accuracy / len(labels)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def lossFunction(labels, predictedLabels):\n",
        "    loss = 0\n",
        "    for l, p in zip(labels, predictedLabels):\n",
        "        loss += (l-p) ** 2\n",
        "    loss /= len(labels)\n",
        "    return loss\n",
        "\n",
        "def cost(qcircuit, params, X, Y):\n",
        "    predictions = [getPrediction(qcircuit, params, x) for x in X]\n",
        "    return lossFunction(Y, predictions)\n",
        "\n",
        "def getPrediction(qcircuit, params, data=None):\n",
        "    \n",
        "    quantumOutput = qcircuit(params, data)\n",
        "    # print(\"For\", data)\n",
        "    # print(\"Prediction: \", quantumOutput)\n",
        "    return quantumOutput\n",
        "    #no bias\n",
        "\n",
        "def makeModel(config):\n",
        "    numQubits = config.numQubits\n",
        "    dev = qml.device(\"qulacs.simulator\", wires=numQubits, shots=1000, gpu=True)\n",
        "    @qml.qnode(dev)\n",
        "    def qcircuit(param, data):\n",
        "        for i in range(config.numLayers):\n",
        "            embeddingCircuit(config, data)\n",
        "            if config.fullEntangle:\n",
        "                for j in range(numQubits):\n",
        "                    for i in range(j):\n",
        "                        qml.CZ(wires=[j,i])\n",
        "            else:\n",
        "                for j in range(numQubits-1):\n",
        "                    qml.CZ(wires=[j,j+1])\n",
        "            for j in range(numQubits):\n",
        "                qml.Rot(param[j][i][0], param[j][i][1], param[j][i][2], wires = [j])\n",
        "            \n",
        "        return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "    def embeddingCircuit(config, data):\n",
        "        norm = np.linalg.norm(data)\n",
        "        norm = norm if norm !=0 else 1 \n",
        "        for i in range(0, len(data)-numQubits, numQubits):\n",
        "            for j in range(numQubits):\n",
        "                qml.RX(data[i+j]*2*math.pi/norm, wires=j)\n",
        "    \n",
        "    return qcircuit\n",
        "\n",
        "\n",
        "def createAndTrain(config, WandB = False):\n",
        "    varInit = 0.01 * np.random.randn(config.numQubits, config.numLayers, 3)\n",
        "    opt = qml.AdamOptimizer()\n",
        "    batchSize = config.batchSize\n",
        "    var = varInit\n",
        "    qcircuit = makeModel(config)\n",
        "    qcircuit(var, X_train[0])\n",
        "    circ = qcircuit.draw()\n",
        "    if WandB:\n",
        "        wandb.log({\"circuit\": str(circ)})\n",
        "\n",
        "    print(\"Sample Circuit:\\n\" , circ)\n",
        "    numBatches = config.numBatches\n",
        "    for it in range(numBatches):\n",
        "        batchIndex = np.random.randint(0, len(X_train), (batchSize,))\n",
        "        X_batch = X_train[batchIndex]\n",
        "        Y_batch = Y_train[batchIndex]\n",
        "        var = opt.step(lambda v: cost(qcircuit, v, X_batch, Y_batch), var)\n",
        "        # Compute accuracy\n",
        "        print(\"Computed batch \", it, var)\n",
        "        predictions = []\n",
        "        if (it%10 == 0):\n",
        "            for x in X_test:\n",
        "                #print(var, x)\n",
        "                op = qcircuit(var, x)\n",
        "                predictions.append(np.sign(op))\n",
        "\n",
        "                #print(qcircuit.draw())\n",
        "            acc = accuracy(Y_test, predictions)\n",
        "            loss = lossFunction(Y_test, predictions)\n",
        "            test_loss = float(loss)\n",
        "            test_acc = float(acc)\n",
        "            predictions = []\n",
        "            if (it%50==0):\n",
        "                for x in X_train:\n",
        "                    op = qcircuit(var, x)\n",
        "                    predictions.append(np.sign(op))\n",
        "                acc = accuracy(Y_train, predictions)\n",
        "                loss = lossFunction(Y_train, predictions)\n",
        "                loss = float(loss)\n",
        "                a = float(acc)\n",
        "                if WandB:\n",
        "                    wandb.log({\"train_cost\":loss, \"train_acc\":a}, commit=False)\n",
        "            if WandB:\n",
        "                wandb.log({\"test_cost\":test_loss, \"test_acc\":test_acc})\n",
        "    if WandB:\n",
        "        wandb.log({\"parameters\": var})\n",
        "    wandb.finish()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHQDzP2H-AJ-"
      },
      "source": [
        "\n",
        "def wandbRun(config):\n",
        "    wandb.init(project='qml-experiments', entity='pj747', config=config)\n",
        "    createAndTrain(config, WandB=True)\n",
        "\n",
        "def wandbSweep():\n",
        "    run = wandb.init()\n",
        "    config = run.config\n",
        "    createAndTrain(config, WandB=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MA9ueUDb-4bG",
        "outputId": "822c00d9-0328-4247-d1b3-492d9569fa3f"
      },
      "source": [
        "wandb.agent('at9tw1qm', function=wandbSweep, entity = 'pj747', project='qml-experiments')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w42bhx97 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdev: default.qubit\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfullEntangle: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumBatches: 400\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumLayers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumQubits: 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.31<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">apricot-sweep-16</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/pj747/qml-experiments\" target=\"_blank\">https://wandb.ai/pj747/qml-experiments</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/pj747/qml-experiments/sweeps/at9tw1qm\" target=\"_blank\">https://wandb.ai/pj747/qml-experiments/sweeps/at9tw1qm</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/pj747/qml-experiments/runs/w42bhx97\" target=\"_blank\">https://wandb.ai/pj747/qml-experiments/runs/w42bhx97</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210529_140145-w42bhx97</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dev' was locked by 'sweep' (ignored update).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Sample Circuit:\n",
            "  0: ──RX(0.0709)──RX(0.456)──RX(0.000336)──RX(0.000302)──RX(0.000584)──RX(0.00124)──RX(0.00891)──RX(2.85e-05)──RX(9.89e-05)──RX(7.28e-05)──RX(0.0753)──RX(0.486)──RX(0.000469)──RX(0.000833)──╭C──RZ(-0.00309)──RY(0.00501)──RZ(-0.00834)──┤ ⟨Z⟩ \n",
            " 1: ──RX(0.0753)──RX(4.16)───RX(0.000222)──RX(0.00022)───RX(0.000188)──RX(0.00312)──RX(0.139)────RX(4.2e-05)───RX(5.52e-05)──RX(6.32e-06)──RX(0.0916)──RX(4.66)───RX(0.000437)──RX(0.000488)──╰Z──RZ(0.00967)───RY(-0.0136)──RZ(0.0138)────┤     \n",
            "\n",
            "Computed batch  0 [[[-0.0130873   0.01501338  0.00166207]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  1 [[[-0.01125778  0.02276141  0.00804505]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  2 [[[-0.01011248  0.02989433  0.01258423]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  3 [[[-0.00590558  0.03731458  0.016072  ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  4 [[[-0.0053497   0.04560096  0.01822845]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  5 [[[-0.00176909  0.0538593   0.0202575 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  6 [[[ 0.00385052  0.06115193  0.0222249 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  7 [[[ 0.01061937  0.06920269  0.02418611]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  8 [[[ 0.01457684  0.07780224  0.02622899]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  9 [[[ 0.01974087  0.08681414  0.0287534 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  10 [[[ 0.02633102  0.09614339  0.03091238]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  11 [[[ 0.03380332  0.10573991  0.03226424]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  12 [[[ 0.03951571  0.11558398  0.0339294 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  13 [[[ 0.04604553  0.12562478  0.03518388]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  14 [[[ 0.05134169  0.13587723  0.03656257]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  15 [[[ 0.05778639  0.14615713  0.03715044]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  16 [[[ 0.06309847  0.1559438   0.03685773]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  17 [[[ 0.06989498  0.16575866  0.03683728]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  18 [[[ 0.07755138  0.17591596  0.03791792]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  19 [[[ 0.08605462  0.18629642  0.03922195]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  20 [[[ 0.09238579  0.19678608  0.03962102]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  21 [[[ 0.09960182  0.2075546   0.0405112 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  22 [[[ 0.10651739  0.21852454  0.04105889]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  23 [[[ 0.11216368  0.22936467  0.04056173]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  24 [[[ 0.11906373  0.24047783  0.03978186]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  25 [[[ 0.12718132  0.25184387  0.03840525]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  26 [[[ 0.13619337  0.26338797  0.03715347]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  27 [[[ 0.14403958  0.2750541   0.03613956]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  28 [[[ 0.15221076  0.2869173   0.03611971]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  29 [[[ 0.16123825  0.29897032  0.0354238 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  30 [[[ 0.16840298  0.31067143  0.0343988 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  31 [[[ 0.17566547  0.32243971  0.03402194]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  32 [[[ 0.18319971  0.33446335  0.0337523 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  33 [[[ 0.19014073  0.34666803  0.0331327 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  34 [[[ 0.19757511  0.35881863  0.03266615]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  35 [[[ 0.20540201  0.3709229   0.03310913]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  36 [[[ 0.21357608  0.38297641  0.03393425]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  37 [[[ 0.22038888  0.39515512  0.03455552]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  38 [[[ 0.22872167  0.40767569  0.03540663]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  39 [[[ 0.23744622  0.42008276  0.03621263]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  40 [[[ 0.24654302  0.43266955  0.03732185]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  41 [[[ 0.25613216  0.44531173  0.03801108]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  42 [[[ 0.26534416  0.45784904  0.03976456]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  43 [[[ 0.27364053  0.4700758   0.04081376]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  44 [[[ 0.28190286  0.48259727  0.04133021]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  45 [[[ 0.29097111  0.49521851  0.04096414]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  46 [[[ 0.29909996  0.50789707  0.04047846]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  47 [[[ 0.3055791   0.52052618  0.03949904]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  48 [[[ 0.31269681  0.5333369   0.03820678]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  49 [[[ 0.31974712  0.54649204  0.03683614]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  50 [[[ 0.32605165  0.55966657  0.03549455]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  51 [[[ 0.3280046   0.5721723   0.03498284]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  52 [[[ 0.33014606  0.58482949  0.03433116]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  53 [[[ 0.33332111  0.59750153  0.03355363]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  54 [[[ 0.33668111  0.60994991  0.03381224]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  55 [[[ 0.34176549  0.62283831  0.03488003]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  56 [[[ 0.34827712  0.63599203  0.03573209]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  57 [[[ 0.35306456  0.64881111  0.03683149]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  58 [[[ 0.3586776   0.66174456  0.03935341]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  59 [[[ 0.36251571  0.67440627  0.04037902]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  60 [[[ 0.36739968  0.68699476  0.04199951]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  61 [[[ 0.37336125  0.69993709  0.04276682]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  62 [[[ 0.37878874  0.71263996  0.04389884]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  63 [[[ 0.38623793  0.72554139  0.04552768]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  64 [[[ 0.39473609  0.73865346  0.04623154]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  65 [[[ 0.4016904   0.7516431   0.04732061]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  66 [[[ 0.4068252   0.76459098  0.04755201]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  67 [[[ 0.40882604  0.77748835  0.04751373]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  68 [[[ 0.41075836  0.79030077  0.04796449]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  69 [[[ 0.41495881  0.80299797  0.04932919]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  70 [[[ 0.42033298  0.81568688  0.05095214]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  71 [[[ 0.42652343  0.82859713  0.05304375]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  72 [[[ 0.43160619  0.84149688  0.0558142 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  73 [[[ 0.43606269  0.85404     0.05638617]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  74 [[[ 0.44186704  0.86675172  0.0578512 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  75 [[[ 0.44809138  0.87916601  0.05920193]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  76 [[[ 0.45348554  0.89125406  0.05999584]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  77 [[[ 0.45821302  0.9033936   0.06095857]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  78 [[[ 0.46352533  0.9153783   0.06196508]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  79 [[[ 0.46839651  0.92694692  0.06272631]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  80 [[[ 0.47362423  0.93902962  0.0636611 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  81 [[[ 0.47700744  0.95055925  0.06411252]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  82 [[[ 0.47821607  0.96205551  0.06433907]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  83 [[[ 0.48029732  0.97343042  0.0647314 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  84 [[[ 0.48313903  0.98505847  0.06593302]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  85 [[[ 0.48731803  0.99692219  0.06675459]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  86 [[[ 0.49142874  1.00871312  0.06833586]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  87 [[[ 0.49521611  1.02038855  0.06963187]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  88 [[[ 0.49874405  1.03203471  0.07133459]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  89 [[[ 0.50185359  1.04349062  0.07303676]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  90 [[[ 0.50494947  1.05496439  0.07420036]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  91 [[[ 0.50710124  1.06601116  0.07507163]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  92 [[[ 0.50758417  1.07725199  0.0768443 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  93 [[[ 0.50825349  1.08849648  0.07911985]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  94 [[[ 0.50847516  1.09999457  0.08150333]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  95 [[[ 0.50815988  1.11166947  0.08352358]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  96 [[[ 0.50867106  1.12346085  0.08577649]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  97 [[[ 0.50911488  1.13521371  0.08837795]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  98 [[[ 0.50756834  1.14716764  0.09071817]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  99 [[[ 0.50542642  1.15916651  0.09329117]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  100 [[[ 0.50382238  1.171172    0.09614671]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  101 [[[ 0.50071696  1.18324696  0.09783701]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  102 [[[ 0.49711816  1.19506799  0.09932889]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  103 [[[ 0.49461486  1.2071388   0.10069104]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  104 [[[ 0.49018912  1.21891813  0.1017251 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  105 [[[ 0.48637384  1.23077567  0.10253715]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  106 [[[ 0.48250906  1.24279879  0.1037654 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  107 [[[ 0.47650119  1.25480987  0.10485607]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  108 [[[ 0.47210275  1.26693366  0.1053913 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  109 [[[ 0.46681346  1.27929389  0.10463532]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  110 [[[ 0.46173447  1.29191832  0.10348199]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  111 [[[ 0.45652012  1.30449861  0.10291913]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  112 [[[ 0.45142947  1.31689467  0.10293371]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  113 [[[ 0.44710015  1.32942001  0.10271554]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  114 [[[ 0.4413796   1.34194092  0.10239018]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  115 [[[ 0.43637379  1.35430696  0.10298348]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  116 [[[ 0.43184088  1.36619132  0.10328054]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  117 [[[ 0.42854635  1.377711    0.10343038]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  118 [[[ 0.42730224  1.38933028  0.10294518]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  119 [[[ 0.42570374  1.40101434  0.10293409]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  120 [[[ 0.42368362  1.41273954  0.10333653]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  121 [[[ 0.42144563  1.42435171  0.10388112]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  122 [[[ 0.42118591  1.43623246  0.10285829]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  123 [[[ 0.42140006  1.4479171   0.10104958]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  124 [[[ 0.42074818  1.45923274  0.09888721]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  125 [[[ 0.41959358  1.47042035  0.09727069]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  126 [[[ 0.41873771  1.48143728  0.09680804]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  127 [[[ 0.41675115  1.49235381  0.0968016 ]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  128 [[[ 0.4167563   1.50302125  0.09704187]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  129 [[[ 0.41904596  1.51361983  0.09663044]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n",
            "Computed batch  130 [[[ 0.4195459   1.5244675   0.09683521]]\n",
            "\n",
            " [[ 0.0096653  -0.01362804  0.01380722]]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}